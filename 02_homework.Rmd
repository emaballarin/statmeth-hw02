---
title: "SMDS Homework - Block 2"
author: "P. Morichetti, L. Arrighi and E. Ballarin | Group 'I'"
date: "22nd April 2020"
output:
  html_document:
    toc: yes
  beamer_presentation:
    highlight: tango
  include: null
  ioslides_presentation:
    highlight: tango
  pdf_document:
    highlight: tango
    keep_tex: yes
    toc: yes
  slide_level: 2
  slidy_presentation:
    fig.height: 3
    fig.width: 4
    highlight: tango
header-includes:
- \usepackage{color}
- \usepackage{graphicx}
- \usepackage{grffile}
- \definecolor{Purple}{HTML}{911146}
- \definecolor{Orange}{HTML}{CF4A30}
- \setbeamercolor{alerted text}{fg=Orange}
- \setbeamercolor{frametitle}{bg=Purple}
institute: University of Trieste, SISSA, ICTP, University of Udine
graphics: yes
fontsize: 10pt
---

```{r global_options, include=FALSE}
knitr::opts_chunk$set(fig.align = 'center', warning=FALSE, message=FALSE, fig.asp=0.625, dev='png', global.par = TRUE, dev.args=list(pointsize=10), fig.path = 'figs/')
library(MASS)
```

```{r setup, include=FALSE}
library(knitr)
local({
  hook_plot = knit_hooks$get('plot')
  knit_hooks$set(plot = function(x, options) {
    paste0('\n\n----\n\n', hook_plot(x, options))
  })
})
```


# ***LAB*** EXERCISES

**Exercise 1**

Check the biased nature of $s^2_b$ via MC simulation, generating $n=10$ iid values from a normal distribution. Plot also $s^2$ and comment the difference.

***Solution***
```{r lab_01, code = readLines("src/lab_01.R"), echo=TRUE}
```
We can observe that maximums of $s^2$ and $s^2_b$ don't coincide, even if $R$ tends to infinity. Due to the fact that $s^2$ is an unbiased estimator, we can conclude that $s^2_b$ is biased. 


**Exercise 2**
Let consider a darts target into $K=4$ zones. Hence we assign the following hitting probabilities:

- Zone 1 (from 1 to 3 points): $p_1=\frac{7}{16}$;

- Zone 2 (from 4 to 6 points); $p_2=\frac{5}{16}$;

- Zone 3 (from 7 to 9 points); $p_3=\frac{3}{16}$;

- Zone 4 (the highest points in the middle of the target, say 10, 25, 50 points): $p_4=\frac{1}{16}$;

The number of players is equal to $7$ and each of them throws $50$ darts, which corresponds to the number of considered observations ($n=50$ for each player). 

We assume as null hypothesis: $\textit{due to a moderate control on his darts skills, he has decreasing probabilities to hit the best zones}$,
$$
H_0 : \,p_1=\frac{7}{16};\,\,\,\,p_2=\frac{5}{16};\,\,\,\,p_3=\frac{3}{16};\,\,\,\,p_4=\frac{1}{16}.
$$
We will try to simulate the data and perform a null-hypothesis-test, assuming that one of the $7$ players is a great player.


***Solution***
```{r lab_02, code = readLines("src/lab_02.R"), echo=TRUE}
```



**Exercise 3**

Consider now some of the most followed Instagram accounts in 2018: for each of the owners, we report also the number of Twitter followers (in milions). Are the Instagram and Twitter account somehow associated? Perform a correlation test, compute the p-value and give an answer. Here is the dataframe.

***Solution***
```{r lab_03, code = readLines("src/lab_03.R"), echo=TRUE}
```




**Exercise 4**

Compute analitically $J(\gamma, \gamma; y)$, $J(\gamma, \beta; y)$, $J(\beta, \beta; y)$.
	
***Solution***

Let $l(\gamma,\beta;y)$ the log-likelihood:	
$$
l(\gamma,\beta;y)=n \log(\gamma)  -  n \gamma \log(\beta)  +  \gamma \sum_{i=1}^n(\log(y_i))  -  \sum_{i=1}^n(y_i/\beta)^\gamma.
$$
Let $J$ the matrix defined as:

$$
J(\theta;y)  =  -\frac{\partial^2 l(\theta;y)}{\partial\theta \partial\theta^T}.
$$
Firstly, we compute the following sums:
	
$$
	\sum_{i=1}^n (y_i/\beta)^\gamma = \beta^{-\gamma}\sum_{i=1}^n y_i^\gamma \ \ \ \ \ \ \mathrm{(a)} \\
$$
$$
	\sum_{i=1}^n (y_i/\beta)^\gamma = \sum_{i=1}^n  e^{\gamma\log(y_i/\beta)} \ \ \mathrm{(b)}
$$
Hence, using $\mathrm{(a)}$ and $\mathrm{(b)}$:
	
$$
\frac{\partial l(\gamma,\beta;y)}{\partial \gamma}  =  \frac{n}{\gamma}  -  n\log(\beta)  +  \sum_{i=1}^n\log(y_i/\beta)^\gamma
$$
$$
	\frac{\partial l(\gamma,\beta;y)}{\partial \beta} = \frac{n\gamma}{\beta}  -  \frac{\gamma}{\beta} \sum_{i=1}^ny^i/\beta
$$
Finally we can compute the partial derivatives:
	
1. $\partial_{\gamma\gamma}^2$ term:
$$
	\frac{\partial^2 l(\gamma,\beta;y)}{\partial\gamma^2}  =  -\frac{n}{\gamma^2}  +  \sum_{i=1}^n \left( \frac{y_i}{\beta} \right)^\gamma \log\left( \frac{y_i}{\beta} \right)^2;
$$
	
2. $\partial_{\gamma\beta}^2$ term:
	
$$
	\frac{\partial^2 l(\gamma,\beta;y)}{\partial\gamma\partial\beta}  =  \frac{1}{\beta} \left[ n  +  \sum_{i=1}^n \left( \frac{y_i}{\beta} \right)^\gamma  +  \gamma\sum_{i=1}^n \left( \frac{y_i}{\beta} \right)^\gamma \log \frac{y_i}{\beta} \right];
$$
	
3. $\partial_{\beta\gamma}^2$ term:
	
$$
	\frac{\partial^2 l(\gamma,\beta;y)}{\partial\beta\partial\gamma}  =  -\frac{n}{\beta}  +  \sum_{i=1}^n \left( \frac{y_i}{\beta} \right)^{\gamma-1}  -  \frac{\gamma}{\beta}\sum_{i=1}^n \left( \frac{y_i}{\beta} \right)^\gamma \log \frac{y_i}{\beta};
$$
	
4. $\partial_{\beta\beta}^2$ term:
	
$$
	\frac{\partial^2 l(\gamma,\beta;y)}{\partial\beta^2}  =  -\frac{n\gamma}{\beta^2}  +  \gamma(\gamma+1)\beta^{-\gamma-2} \sum_{i=1}^ny_i^\gamma.
$$

**Exercise 5**

Produce the contour plot for the quadratic approximation of the log-likelihood, based on the Taylor series:
$$
l(\theta)-l(\hat{\theta}) \approx -\frac{1}{2}(\theta-\hat{\theta})^T\,J(\hat{\theta})(\theta-\hat{\theta}).
$$

***Solution***
```{r lab_05, code = readLines("src/lab_05.R"), echo=TRUE}
```


# ***DAAG*** EXERCISES

**Exercise 1**

- Text.

- Text.

***Solution***
#```{r lab_01, code = readLines("src/lab_01.R"), echo=TRUE}
#```


# ***CS*** EXERCISES

**Exercise 1**

- Text.

- Text.

***Solution***
#```{r lab_01, code = readLines("src/lab_01.R"), echo=TRUE}
#```
